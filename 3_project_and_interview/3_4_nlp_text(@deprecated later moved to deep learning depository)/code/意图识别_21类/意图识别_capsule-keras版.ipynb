{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:44:10.713158Z",
     "start_time": "2018-10-23T11:44:07.976944Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from m import  BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:44:10.894246Z",
     "start_time": "2018-10-23T11:44:10.714933Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "K.clear_session()\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "K.clear_session()\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:44:10.898456Z",
     "start_time": "2018-10-23T11:44:10.895753Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "maxlen = 30\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:44:23.945600Z",
     "start_time": "2018-10-23T11:44:10.899950Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Word Count: 100%|██████████| 53850/53850 [00:00<00:00, 433771.02it/s]\n",
      "Doc To Number: 100%|██████████| 53850/53850 [00:00<00:00, 151585.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# 以训练数据为例\n",
    "data_path_dir = 'data'\n",
    "data = pd.read_csv(os.path.join(data_path_dir,'cuishou_intent3.csv'),sep='\\t')\n",
    "data.columns = ['content','label']\n",
    "\n",
    "data_tmp = data.copy(deep=True)\n",
    "\n",
    "d_ = {}\n",
    "for key, value in enumerate(set(data_tmp.label)):\n",
    "    d_[value] = key\n",
    "data_tmp['label'] = data_tmp['label'].apply(lambda x : d_.get(x))\n",
    "\n",
    "y_train = np.array(data_tmp.label.tolist())\n",
    "# 构造embedding字典\n",
    "bow = BOW(data_tmp.content.apply(jieba.lcut).tolist(), min_count=1, maxlen=30) # 长度补齐或截断固定长度30\n",
    "\n",
    "vocab_size = len(bow.word2idx)\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('data/ft_wv.txt')\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))\n",
    "for key, value in bow.word2idx.items():\n",
    "    if key in word2vec.vocab: # Word2Vec训练得到的的实例需要word2vec.wv.vocab\n",
    "        embedding_matrix[value] = word2vec.get_vector(key)\n",
    "    else:\n",
    "        embedding_matrix[value] = [0] * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:44:26.040402Z",
     "start_time": "2018-10-23T11:44:26.022792Z"
    }
   },
   "outputs": [],
   "source": [
    "# label要处理成one-hot的形式\n",
    "X_train = copy.deepcopy(bow.doc2num)\n",
    "y_train = copy.deepcopy(y_train)\n",
    "y_train = np.array(pd.get_dummies(y_train)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:45:17.375989Z",
     "start_time": "2018-10-23T11:45:17.362807Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_capsule_model():\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(vocab_size + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(21, activation='softmax')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:54:52.204704Z",
     "start_time": "2018-10-23T11:54:52.156042Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "for train_idx, val_idx in skf.split(X_train, np.argmax(y_train,axis=1)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T11:56:09.735635Z",
     "start_time": "2018-10-23T11:56:09.716577Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_1 = copy.deepcopy(X_train[train_idx])\n",
    "y_train_1 = copy.deepcopy(y_train[train_idx])\n",
    "X_val = X_train[val_idx]\n",
    "y_val = y_train[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-23T12:04:19.331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43088 samples, validate on 10762 samples\n",
      "Epoch 1/15\n",
      "43088/43088 [==============================] - 31s 724us/step - loss: 0.3629 - acc: 0.9141 - val_loss: 0.1397 - val_acc: 0.9622\n",
      "Epoch 2/15\n",
      "43088/43088 [==============================] - 30s 699us/step - loss: 0.1120 - acc: 0.9702 - val_loss: 0.0790 - val_acc: 0.9799\n",
      "Epoch 3/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0801 - acc: 0.9778 - val_loss: 0.0604 - val_acc: 0.9831\n",
      "Epoch 4/15\n",
      "43088/43088 [==============================] - 30s 687us/step - loss: 0.0658 - acc: 0.9822 - val_loss: 0.0499 - val_acc: 0.9868\n",
      "Epoch 5/15\n",
      "43088/43088 [==============================] - 29s 680us/step - loss: 0.0548 - acc: 0.9846 - val_loss: 0.0510 - val_acc: 0.9856\n",
      "Epoch 6/15\n",
      "43088/43088 [==============================] - 29s 683us/step - loss: 0.0509 - acc: 0.9852 - val_loss: 0.0444 - val_acc: 0.9880\n",
      "Epoch 7/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0438 - acc: 0.9875 - val_loss: 0.0397 - val_acc: 0.9879\n",
      "Epoch 8/15\n",
      "43088/43088 [==============================] - 30s 687us/step - loss: 0.0403 - acc: 0.9883 - val_loss: 0.0415 - val_acc: 0.9880\n",
      "Epoch 9/15\n",
      "43088/43088 [==============================] - 29s 679us/step - loss: 0.0363 - acc: 0.9893 - val_loss: 0.0395 - val_acc: 0.9884\n",
      "Epoch 10/15\n",
      "43088/43088 [==============================] - 29s 675us/step - loss: 0.0332 - acc: 0.9903 - val_loss: 0.0348 - val_acc: 0.9895\n",
      "Epoch 11/15\n",
      "43088/43088 [==============================] - 29s 677us/step - loss: 0.0315 - acc: 0.9904 - val_loss: 0.0369 - val_acc: 0.9888\n",
      "Epoch 12/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0313 - acc: 0.9906 - val_loss: 0.0362 - val_acc: 0.9884\n",
      "Epoch 13/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0290 - acc: 0.9915 - val_loss: 0.0312 - val_acc: 0.9902\n",
      "Epoch 14/15\n",
      "43088/43088 [==============================] - 29s 669us/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.0305 - val_acc: 0.9909\n",
      "Epoch 15/15\n",
      "43088/43088 [==============================] - 29s 681us/step - loss: 0.0261 - acc: 0.9920 - val_loss: 0.0324 - val_acc: 0.9906\n",
      "Train on 43088 samples, validate on 10762 samples\n",
      "Epoch 1/15\n",
      "43088/43088 [==============================] - 31s 714us/step - loss: 0.3746 - acc: 0.9113 - val_loss: 0.1174 - val_acc: 0.9707\n",
      "Epoch 2/15\n",
      "43088/43088 [==============================] - 30s 687us/step - loss: 0.1111 - acc: 0.9708 - val_loss: 0.0789 - val_acc: 0.9778\n",
      "Epoch 3/15\n",
      "43088/43088 [==============================] - 29s 682us/step - loss: 0.0792 - acc: 0.9787 - val_loss: 0.0651 - val_acc: 0.9810\n",
      "Epoch 4/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0650 - acc: 0.9818 - val_loss: 0.0581 - val_acc: 0.9835\n",
      "Epoch 5/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0566 - acc: 0.9837 - val_loss: 0.0494 - val_acc: 0.9857\n",
      "Epoch 6/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0486 - acc: 0.9864 - val_loss: 0.0464 - val_acc: 0.9874\n",
      "Epoch 7/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0428 - acc: 0.9877 - val_loss: 0.0397 - val_acc: 0.9886\n",
      "Epoch 8/15\n",
      "43088/43088 [==============================] - 29s 674us/step - loss: 0.0385 - acc: 0.9891 - val_loss: 0.0392 - val_acc: 0.9888\n",
      "Epoch 9/15\n",
      "43088/43088 [==============================] - 29s 672us/step - loss: 0.0384 - acc: 0.9891 - val_loss: 0.0419 - val_acc: 0.9875\n",
      "Epoch 10/15\n",
      "43088/43088 [==============================] - 29s 679us/step - loss: 0.0345 - acc: 0.9899 - val_loss: 0.0452 - val_acc: 0.9867\n",
      "Epoch 11/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0328 - acc: 0.9905 - val_loss: 0.0327 - val_acc: 0.9903\n",
      "Epoch 12/15\n",
      "43088/43088 [==============================] - 30s 686us/step - loss: 0.0306 - acc: 0.9905 - val_loss: 0.0308 - val_acc: 0.9906\n",
      "Epoch 13/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0274 - acc: 0.9919 - val_loss: 0.0334 - val_acc: 0.9899\n",
      "Epoch 14/15\n",
      "43088/43088 [==============================] - 29s 669us/step - loss: 0.0239 - acc: 0.9928 - val_loss: 0.0315 - val_acc: 0.9902\n",
      "Epoch 15/15\n",
      "43088/43088 [==============================] - 29s 673us/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.0304 - val_acc: 0.9896\n",
      "Train on 43088 samples, validate on 10762 samples\n",
      "Epoch 1/15\n",
      "43088/43088 [==============================] - 31s 710us/step - loss: 0.3689 - acc: 0.9132 - val_loss: 0.1226 - val_acc: 0.9705\n",
      "Epoch 2/15\n",
      "43088/43088 [==============================] - 29s 679us/step - loss: 0.1153 - acc: 0.9693 - val_loss: 0.0786 - val_acc: 0.9796\n",
      "Epoch 3/15\n",
      "43088/43088 [==============================] - 29s 684us/step - loss: 0.0810 - acc: 0.9778 - val_loss: 0.0675 - val_acc: 0.9810\n",
      "Epoch 4/15\n",
      "43088/43088 [==============================] - 29s 674us/step - loss: 0.0654 - acc: 0.9818 - val_loss: 0.0583 - val_acc: 0.9830\n",
      "Epoch 5/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0577 - acc: 0.9836 - val_loss: 0.0502 - val_acc: 0.9858\n",
      "Epoch 6/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0499 - acc: 0.9858 - val_loss: 0.0457 - val_acc: 0.9876\n",
      "Epoch 7/15\n",
      "43088/43088 [==============================] - 29s 677us/step - loss: 0.0459 - acc: 0.9868 - val_loss: 0.0433 - val_acc: 0.9869\n",
      "Epoch 8/15\n",
      "43088/43088 [==============================] - 29s 670us/step - loss: 0.0408 - acc: 0.9880 - val_loss: 0.0411 - val_acc: 0.9875\n",
      "Epoch 9/15\n",
      "43088/43088 [==============================] - 29s 670us/step - loss: 0.0374 - acc: 0.9892 - val_loss: 0.0393 - val_acc: 0.9885\n",
      "Epoch 10/15\n",
      "43088/43088 [==============================] - 29s 670us/step - loss: 0.0358 - acc: 0.9895 - val_loss: 0.0401 - val_acc: 0.9877\n",
      "Epoch 11/15\n",
      "43088/43088 [==============================] - 29s 681us/step - loss: 0.0348 - acc: 0.9897 - val_loss: 0.0346 - val_acc: 0.9895\n",
      "Epoch 12/15\n",
      "43088/43088 [==============================] - 29s 679us/step - loss: 0.0297 - acc: 0.9913 - val_loss: 0.0366 - val_acc: 0.9888\n",
      "Epoch 13/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0283 - acc: 0.9914 - val_loss: 0.0363 - val_acc: 0.9896\n",
      "Epoch 14/15\n",
      "43088/43088 [==============================] - 29s 674us/step - loss: 0.0263 - acc: 0.9918 - val_loss: 0.0306 - val_acc: 0.9910\n",
      "Epoch 15/15\n",
      "43088/43088 [==============================] - 29s 679us/step - loss: 0.0248 - acc: 0.9924 - val_loss: 0.0326 - val_acc: 0.9898\n",
      "Train on 43088 samples, validate on 10762 samples\n",
      "Epoch 1/15\n",
      "43088/43088 [==============================] - 31s 711us/step - loss: 0.3748 - acc: 0.9114 - val_loss: 0.1160 - val_acc: 0.9705\n",
      "Epoch 2/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.1126 - acc: 0.9698 - val_loss: 0.0726 - val_acc: 0.9816\n",
      "Epoch 3/15\n",
      "43088/43088 [==============================] - 29s 674us/step - loss: 0.0806 - acc: 0.9778 - val_loss: 0.0617 - val_acc: 0.9823\n",
      "Epoch 4/15\n",
      "43088/43088 [==============================] - 29s 673us/step - loss: 0.0649 - acc: 0.9829 - val_loss: 0.0572 - val_acc: 0.9836\n",
      "Epoch 5/15\n",
      "43088/43088 [==============================] - 29s 675us/step - loss: 0.0562 - acc: 0.9839 - val_loss: 0.0463 - val_acc: 0.9862\n",
      "Epoch 6/15\n",
      "43088/43088 [==============================] - 30s 693us/step - loss: 0.0473 - acc: 0.9864 - val_loss: 0.0449 - val_acc: 0.9870\n",
      "Epoch 7/15\n",
      "43088/43088 [==============================] - 29s 682us/step - loss: 0.0431 - acc: 0.9874 - val_loss: 0.0387 - val_acc: 0.9884\n",
      "Epoch 8/15\n",
      "43088/43088 [==============================] - 30s 688us/step - loss: 0.0413 - acc: 0.9879 - val_loss: 0.0415 - val_acc: 0.9889\n",
      "Epoch 9/15\n",
      "43088/43088 [==============================] - 29s 675us/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.0398 - val_acc: 0.9895\n",
      "Epoch 10/15\n",
      "43088/43088 [==============================] - 29s 674us/step - loss: 0.0337 - acc: 0.9903 - val_loss: 0.0380 - val_acc: 0.9889\n",
      "Epoch 11/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0341 - acc: 0.9899 - val_loss: 0.0367 - val_acc: 0.9884\n",
      "Epoch 12/15\n",
      "43088/43088 [==============================] - 29s 676us/step - loss: 0.0307 - acc: 0.9914 - val_loss: 0.0344 - val_acc: 0.9905\n",
      "Epoch 13/15\n",
      "43088/43088 [==============================] - 30s 686us/step - loss: 0.0267 - acc: 0.9922 - val_loss: 0.0349 - val_acc: 0.9897\n",
      "Epoch 14/15\n",
      "43088/43088 [==============================] - 30s 689us/step - loss: 0.0266 - acc: 0.9915 - val_loss: 0.0318 - val_acc: 0.9910\n",
      "Epoch 15/15\n",
      "43088/43088 [==============================] - 29s 672us/step - loss: 0.0250 - acc: 0.9924 - val_loss: 0.0310 - val_acc: 0.9904\n",
      "Train on 43088 samples, validate on 10762 samples\n",
      "Epoch 1/15\n",
      "43088/43088 [==============================] - 31s 708us/step - loss: 0.3697 - acc: 0.9105 - val_loss: 0.1289 - val_acc: 0.9654\n",
      "Epoch 2/15\n",
      "43088/43088 [==============================] - 29s 683us/step - loss: 0.1161 - acc: 0.9685 - val_loss: 0.0824 - val_acc: 0.9757\n",
      "Epoch 3/15\n",
      "43088/43088 [==============================] - 29s 683us/step - loss: 0.0803 - acc: 0.9778 - val_loss: 0.0625 - val_acc: 0.9816\n",
      "Epoch 4/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0662 - acc: 0.9810 - val_loss: 0.0559 - val_acc: 0.9846\n",
      "Epoch 5/15\n",
      "43088/43088 [==============================] - 29s 674us/step - loss: 0.0576 - acc: 0.9837 - val_loss: 0.0489 - val_acc: 0.9869\n",
      "Epoch 6/15\n",
      "43088/43088 [==============================] - 29s 675us/step - loss: 0.0503 - acc: 0.9858 - val_loss: 0.0460 - val_acc: 0.9871\n",
      "Epoch 7/15\n",
      "43088/43088 [==============================] - 29s 680us/step - loss: 0.0454 - acc: 0.9869 - val_loss: 0.0415 - val_acc: 0.9878\n",
      "Epoch 8/15\n",
      "43088/43088 [==============================] - 29s 681us/step - loss: 0.0401 - acc: 0.9883 - val_loss: 0.0490 - val_acc: 0.9849\n",
      "Epoch 9/15\n",
      "43088/43088 [==============================] - 29s 677us/step - loss: 0.0418 - acc: 0.9876 - val_loss: 0.0398 - val_acc: 0.9877\n",
      "Epoch 10/15\n",
      "43088/43088 [==============================] - 29s 675us/step - loss: 0.0352 - acc: 0.9896 - val_loss: 0.0393 - val_acc: 0.9885\n",
      "Epoch 11/15\n",
      "43088/43088 [==============================] - 29s 678us/step - loss: 0.0348 - acc: 0.9892 - val_loss: 0.0386 - val_acc: 0.9893\n",
      "Epoch 12/15\n",
      "43088/43088 [==============================] - 30s 689us/step - loss: 0.0309 - acc: 0.9912 - val_loss: 0.0347 - val_acc: 0.9900\n",
      "Epoch 13/15\n",
      "43088/43088 [==============================] - 29s 679us/step - loss: 0.0287 - acc: 0.9920 - val_loss: 0.0350 - val_acc: 0.9896\n",
      "Epoch 14/15\n",
      "43088/43088 [==============================] - 29s 677us/step - loss: 0.0294 - acc: 0.9911 - val_loss: 0.0321 - val_acc: 0.9905\n",
      "Epoch 15/15\n",
      "43088/43088 [==============================] - 30s 686us/step - loss: 0.0244 - acc: 0.9927 - val_loss: 0.0310 - val_acc: 0.9913\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model = get_capsule_model()\n",
    "    hist = model.fit(X_train_1, y_train_1, validation_data=(X_val,y_val), batch_size=128, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.12892536918396091, 0.08242203289415508, 0.062457309024714115, 0.05588281770091233, 0.048920187518994754, 0.04596260152175632, 0.04150009806749242, 0.04901023257400545, 0.03982942132305238, 0.03930488572838614, 0.038571269717154015, 0.03469154188439563, 0.03497855218819328, 0.032119513189018914, 0.03098908124210843], 'val_acc': [0.9654339341908178, 0.9756550826762295, 0.9816019327041053, 0.9845753577180433, 0.986898346032336, 0.9870841850735533, 0.9878275413270379, 0.9849470358447855, 0.987734621817506, 0.9884779780709905, 0.9893142538340068, 0.9899646905779594, 0.9895930124512172, 0.9905222077459191, 0.9912655640215573], 'loss': [0.36973854530889044, 0.116115629270457, 0.08031857037218892, 0.06620479237206621, 0.0575846308517912, 0.05027253980026607, 0.04535247760693789, 0.04014299534305493, 0.041774897132469346, 0.03520113558636759, 0.03484118176777036, 0.03092092395553658, 0.028736512534540994, 0.02944773405332449, 0.024414141373650052], 'acc': [0.9104623096696604, 0.9684831043445971, 0.9778360564868954, 0.9810155960117358, 0.9836845524993653, 0.9857965094911268, 0.9869337170441886, 0.9883030078422611, 0.9876299665800223, 0.9895562570067618, 0.9891849238767174, 0.991204047530635, 0.9919931303379131, 0.9911112142591905, 0.9927357965316022]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
