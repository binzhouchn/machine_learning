{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:21.438125Z",
     "start_time": "2018-10-19T03:45:18.811236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from m import f1_for_car, BOW, BasicModule\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# capsule.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建模型 capsule.py\n",
    "# from capsule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "K.clear_session()\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "K.clear_session()\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:21.588744Z",
     "start_time": "2018-10-19T03:45:21.439851Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "remove_stop_words = False\n",
    "\n",
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test_public.csv'\n",
    "data = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "data['label'] = data['subject'] + data['sentiment_value'].astype(str)\n",
    "subj_lst = list(filter(lambda x : x is not np.nan, list(set(data.label))))\n",
    "subj_lst_dic = {value:key for key, value in enumerate(subj_lst)}\n",
    "data['label'] = data['label'].apply(lambda x : subj_lst_dic.get(x))\n",
    "\n",
    "data['content'] = data.content.map(lambda x: ''.join(x.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:33.730125Z",
     "start_time": "2018-10-19T03:45:21.591050Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.149 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "   Word Count: 100%|██████████| 9947/9947 [00:00<00:00, 88261.81it/s]\n",
      "Doc To Number: 100%|██████████| 9947/9947 [00:00<00:00, 70597.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# embeddings_index\n",
    "bow = BOW(data.content.apply(jieba.lcut).tolist(), min_count=1, maxlen=100)\n",
    "\n",
    "word2vec = Word2Vec(data.content.apply(jieba.lcut).tolist(),size=300,min_count=1)\n",
    "\n",
    "embeddings_index = {}\n",
    "def get_word_embed_dict():\n",
    "    for i in word2vec.wv.vocab:\n",
    "        embeddings_index[i] = word2vec.wv.get_vector(i).tolist()\n",
    "#     embeddings_index['UNK'] = [0]*300\n",
    "    return embeddings_index\n",
    "word_embed_dict = get_word_embed_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:34.989017Z",
     "start_time": "2018-10-19T03:45:33.731835Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for ind, row in data.iterrows():\n",
    "    content, label = row['content'], row['label']\n",
    "    if train_dict.get(content) is None:\n",
    "        train_dict[content] = set([label])\n",
    "    else:\n",
    "        train_dict[content].add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:34.997898Z",
     "start_time": "2018-10-19T03:45:34.990651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conts = []\n",
    "labels = []\n",
    "for k, v in train_dict.items():\n",
    "    conts.append(k)\n",
    "    labels.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:35.017142Z",
     "start_time": "2018-10-19T03:45:34.999414Z"
    }
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:37.963534Z",
     "start_time": "2018-10-19T03:45:35.018639Z"
    }
   },
   "outputs": [],
   "source": [
    "content_list = [jieba.lcut(str(c)) for c in conts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:38.870585Z",
     "start_time": "2018-10-19T03:45:37.965181Z"
    }
   },
   "outputs": [],
   "source": [
    "test_content_list = [jieba.lcut(c) for c in test_df.content.astype(str).values]\n",
    "word_set = set([word for row in list(content_list) + list(test_content_list) for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:38.947595Z",
     "start_time": "2018-10-19T03:45:38.872241Z"
    }
   },
   "outputs": [],
   "source": [
    "word2index = {w: i + 1 for i, w in enumerate(word_set)}\n",
    "seqs = [[word2index[w] for w in l] for l in content_list]\n",
    "seqs_dev = [[word2index[w] for w in l] for l in test_content_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:39.202109Z",
     "start_time": "2018-10-19T03:45:38.949210Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM= 300\n",
    "embedding_matrix = np.zeros((len(word2index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word2index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "max_features = len(word_set) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T03:45:39.312675Z",
     "start_time": "2018-10-19T03:45:39.207648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8290, 100) (2364, 100) (8290, 30)\n"
     ]
    }
   ],
   "source": [
    "def get_padding_data(maxlen=100):\n",
    "    x_train = sequence.pad_sequences(seqs, maxlen=maxlen)\n",
    "    x_dev = sequence.pad_sequences(seqs_dev, maxlen=maxlen)\n",
    "    return x_train, x_dev\n",
    "maxlen = 100\n",
    "X_train, X_dev = get_padding_data(maxlen)\n",
    "print(X_train.shape, X_dev.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T07:33:12.930481Z",
     "start_time": "2018-10-19T07:33:12.916937Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_capsule_model():\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(len(word2index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(30, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T08:25:38.879458Z",
     "start_time": "2018-10-19T08:12:32.133358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.W  (1, 256, 160)\n",
      "u_hat_vecs before  (None, None, 160)\n",
      "u_hat_vecs after1  (None, None, 10, 16)\n",
      "Epoch 1/15\n",
      "8290/8290 [==============================] - 26s 3ms/step - loss: 0.2309 - acc: 0.9513\n",
      "Epoch 2/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1505 - acc: 0.9603\n",
      "Epoch 3/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1406 - acc: 0.9622\n",
      "Epoch 4/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1329 - acc: 0.9635\n",
      "Epoch 5/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1278 - acc: 0.9644\n",
      "Epoch 6/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1242 - acc: 0.9647\n",
      "Epoch 7/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1205 - acc: 0.9656\n",
      "Epoch 8/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1183 - acc: 0.9658\n",
      "Epoch 9/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1161 - acc: 0.9661\n",
      "Epoch 10/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1137 - acc: 0.9666\n",
      "Epoch 11/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1122 - acc: 0.9667\n",
      "Epoch 12/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1104 - acc: 0.9668\n",
      "Epoch 13/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1094 - acc: 0.9671\n",
      "Epoch 14/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1077 - acc: 0.9673\n",
      "Epoch 15/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1066 - acc: 0.9675\n",
      "self.W  (1, 256, 160)\n",
      "u_hat_vecs before  (None, None, 160)\n",
      "u_hat_vecs after1  (None, None, 10, 16)\n",
      "Epoch 1/15\n",
      "8290/8290 [==============================] - 26s 3ms/step - loss: 0.2281 - acc: 0.9513\n",
      "Epoch 2/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1510 - acc: 0.9600\n",
      "Epoch 3/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1419 - acc: 0.9617\n",
      "Epoch 4/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1348 - acc: 0.9629\n",
      "Epoch 5/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1289 - acc: 0.9639\n",
      "Epoch 6/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1246 - acc: 0.9647\n",
      "Epoch 7/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1207 - acc: 0.9655\n",
      "Epoch 8/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1176 - acc: 0.9658\n",
      "Epoch 9/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1146 - acc: 0.9661\n",
      "Epoch 10/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1132 - acc: 0.9663\n",
      "Epoch 11/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1104 - acc: 0.9667\n",
      "Epoch 12/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1089 - acc: 0.9669\n",
      "Epoch 13/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1071 - acc: 0.9676\n",
      "Epoch 14/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1053 - acc: 0.9675\n",
      "Epoch 15/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1043 - acc: 0.9677\n",
      "self.W  (1, 256, 160)\n",
      "u_hat_vecs before  (None, None, 160)\n",
      "u_hat_vecs after1  (None, None, 10, 16)\n",
      "Epoch 1/15\n",
      "8290/8290 [==============================] - 26s 3ms/step - loss: 0.2279 - acc: 0.9518\n",
      "Epoch 2/15\n",
      "8290/8290 [==============================] - 24s 3ms/step - loss: 0.1493 - acc: 0.9605\n",
      "Epoch 3/15\n",
      "1536/8290 [====>.........................] - ETA: 19s - loss: 0.1385 - acc: 0.9626"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-f85b5dc98ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_capsule_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfirst_model_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_model_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/algor/zhoubin/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "first_model_results = []\n",
    "for i in range(5):\n",
    "    model = get_capsule_model()\n",
    "    model.fit(X_train, y_train, batch_size=64, epochs=15)\n",
    "    first_model_results.append(model.predict(X_dev, batch_size=1024))\n",
    "pred4 = np.average(first_model_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:32:59.910636Z",
     "start_time": "2018-10-19T02:32:59.846010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat_vecs (64, 10, 100, 16)\n",
      "b (64, 100, 10)\n",
      "c (64, 10, 100)\n",
      "(64, 10, 16)\n",
      "(64, 10, 100)\n"
     ]
    }
   ],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "u_hat_vecs = K.ones(shape=(64,100,10,16))\n",
    "u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "print('u_hat_vecs', K.int_shape(u_hat_vecs))\n",
    "b = K.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "b = K.permute_dimensions(b, (0, 2, 1))\n",
    "print('b', K.int_shape(b) )\n",
    "c = K.softmax(b)\n",
    "c = K.permute_dimensions(c, (0, 2, 1))\n",
    "b = K.permute_dimensions(b, (0, 2, 1))\n",
    "print('c', K.int_shape(c))\n",
    "\n",
    "outputs = squash(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "print(outputs.shape)\n",
    "b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T02:33:31.348777Z",
     "start_time": "2018-10-19T02:33:31.330402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat_vecs torch.Size([64, 10, 100, 16])\n",
      "c torch.Size([64, 10, 100])\n",
      "outputs torch.Size([64, 10, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/algor/zhoubin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 100])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_hat_vecs =torch.ones(64,100,10,16)\n",
    "u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)\n",
    "print('u_hat_vecs', u_hat_vecs.size())\n",
    "b = torch.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "b = b.permute(0, 2, 1)\n",
    "c = nn.functional.softmax(b)\n",
    "c = c.permute(0, 2, 1)\n",
    "b = b.permute(0, 2, 1)\n",
    "print('c', c.size())\n",
    "\n",
    "outputs = torch.einsum('bij,bijk->bik', (c, u_hat_vecs)) # batch matrix multiplication\n",
    "print('outputs',outputs.size())\n",
    "\n",
    "torch.einsum('bik,bijk->bij', (outputs, u_hat_vecs)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T08:35:02.535259Z",
     "start_time": "2018-10-19T08:35:01.003640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (2, 3, 2)\n",
      "w (1, 2, 4)\n",
      "res (2, 1, 4)\n",
      "x_ (3, 2)\n",
      "w_ (2, 4)\n",
      "[[[ 0.65999997  8.         -2.9         3.6       ]]\n",
      "\n",
      " [[ 1.19       12.1        -3.8         7.4999995 ]]]\n",
      "------------------------------\n",
      "[[  1.19       12.1        -3.8         7.4999995]\n",
      " [  3.31       28.500002   -7.3999996  23.099998 ]\n",
      " [  2.24       31.800001  -12.7        10.199999 ]]\n"
     ]
    }
   ],
   "source": [
    "x = K.constant(np.array([[[1,2],[5,6],[3,9]],[[2,3],[6,7],[3,8]]]))\n",
    "w = K.constant(np.array([[[0.4,0.2,1.1,4.2],[0.13,3.9,-2,-0.3]]]))\n",
    "# res = K.conv1d(x,w)\n",
    "res = K.local_conv1d(x,w,[1],[1])\n",
    "print('x',x.shape)\n",
    "print('w',w.shape)\n",
    "print('res',res.shape)\n",
    "x_ = x[1,:,:]\n",
    "w_ = w[0,:,:]\n",
    "print('x_',x_.shape)\n",
    "print('w_',w_.shape)\n",
    "res2 = K.dot(x_,w_)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    cc = sess.run(res)\n",
    "    print(cc)\n",
    "    print('------------------------------')\n",
    "    dd = sess.run(res2)\n",
    "    print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}