{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "from m import f1_for_car, BOW, BasicModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题和情感合起来变成30类\n",
    "data['subject_1'] = data['subject'] + data['sentiment_value'].astype(str)\n",
    "subj_lst = list(filter(lambda x : x is not np.nan, list(set(data.subject_1))))\n",
    "subj_lst_dic = {value:key for key, value in enumerate(subj_lst)}\n",
    "data['label'] = data['subject_1'].apply(lambda x : subj_lst_dic.get(x))\n",
    "\n",
    "data = data[['content', 'label']].copy(deep=True)\n",
    "data_tmp = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "USE_CUDA=True\n",
    "EPOCH = 30           # 训练整批数据多少次\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.002         # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = {}\n",
    "for key, value in enumerate(set(data_tmp.label)):\n",
    "    d_[value] = key\n",
    "data_tmp['label'] = data_tmp['label'].apply(lambda x : d_.get(x))\n",
    "\n",
    "y_train = np.array(data_tmp.label.tolist())\n",
    "# 构造embedding字典\n",
    "bow = BOW(data_tmp.content.apply(jieba.lcut).tolist(), min_count=1, maxlen=30) # 长度补齐或截断固定长度30\n",
    "\n",
    "vocab_size = len(bow.word2idx)\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('data/ft_wv.txt')\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size+1, 300))\n",
    "for key, value in bow.word2idx.items():\n",
    "    if key in word2vec.vocab: # Word2Vec训练得到的的实例需要word2vec.wv.vocab\n",
    "        embedding_matrix[value] = word2vec.get_vector(key)\n",
    "    else:\n",
    "        embedding_matrix[value] = [0] * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('save/embedding_matrix',arr=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word对应的index\n",
    "X_train = copy.deepcopy(bow.doc2num)\n",
    "y_train = copy.deepcopy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "\n",
    "    loss = 'multilabelloss'\n",
    "    model='LSTMText' \n",
    "    title_dim = 100 # 标题的卷积核数\n",
    "    content_dim = 100 #内容的卷积核数\n",
    "    num_classes = 30 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 1000 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2 # k\n",
    "    hidden_size = 128 #LSTM hidden size\n",
    "    num_layers=2 #LSTM layers\n",
    "    inception_dim = 256 #inception的卷积核数\n",
    "    \n",
    "    kernel_size = 3 #单尺度卷积核\n",
    "    kernel_sizes = [2,3,4] #多尺度卷积核\n",
    "    # vocab_size = 11973 # num of chars\n",
    "    vocab_size = vocab_size# num of words \n",
    "    content_seq_len = 50 #内容长度 word为50 char为100\n",
    "    static = False\n",
    "    embedding_path = 'save/embedding_matrix.npy'\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes =  [1,2,3,4]\n",
    "class MultiCNNTextBNDeep(BasicModule): \n",
    "    def __init__(self, opt ):\n",
    "        super(MultiCNNTextBNDeep, self).__init__()\n",
    "        self.model_name = 'MultiCNNTextBNDeep'\n",
    "        self.opt=opt\n",
    "        self.encoder = nn.Embedding(self.opt.vocab_size+1, opt.embedding_dim)\n",
    "\n",
    "        content_convs = [nn.Sequential(\n",
    "                                nn.Conv1d(in_channels = self.opt.embedding_dim,\n",
    "                                        out_channels = self.opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(self.opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "\n",
    "                                nn.Conv1d(in_channels = self.opt.content_dim,\n",
    "                                        out_channels = self.opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(self.opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                # maxpool1d kernel_size=50的意思就是对一句话里每50个单词取maxpool\n",
    "                                nn.MaxPool1d(kernel_size = (self.opt.content_seq_len - kernel_size*2 + 2))\n",
    "                            )\n",
    "            for kernel_size in kernel_sizes]\n",
    "\n",
    "        self.content_convs = nn.ModuleList(content_convs)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(len(kernel_sizes)*self.opt.content_dim,self.opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(self.opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.opt.linear_hidden_size,self.opt.num_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "\n",
    "        if opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(self.opt.embedding_path)))\n",
    "\n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "        if self.opt.static:\n",
    "            content.detach()\n",
    "        \n",
    "        \n",
    "        content_out = [content_conv(content.permute(0,2,1)) for content_conv in self.content_convs]\n",
    "#         conv_out = t.cat((title_out+content_out),dim=1)\n",
    "        # t.cat是对list进行拼接，这里对维度1进行拼接\n",
    "        conv_out = t.cat(content_out,dim=1)\n",
    "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
    "        softmax = self.fc((reshaped))\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理成tensor\n",
    "label_tensor = torch.from_numpy(np.array(y_train)).long()\n",
    "content_tensor = torch.from_numpy(np.array(X_train)).long()\n",
    "\n",
    "torch_dataset = Data.TensorDataset(content_tensor, label_tensor)\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # random shuffle for training\n",
    "        num_workers=8,              # subprocesses for loading data\n",
    "    )\n",
    "\n",
    "# 如果需要验证集则可以将X_train进行拆分\n",
    "\n",
    "# model, optimizer, loss_func\n",
    "m = MultiCNNTextBNDeep(opt)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=LR)   # optimize all lstm parameters;Adam比较好用\n",
    "loss_func = torch.nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "if USE_CUDA:\n",
    "    m.cuda()\n",
    "    loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # val\n",
    "# if USE_CUDA:\n",
    "#     content_val_tensor = content_val_tensor.cuda()\n",
    "#     label_val_tensor = label_val_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 1\n",
    "for epoch in tqdm_notebook(range(EPOCH)):\n",
    "    for step, (content, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "        content, b_y = content.cuda(), b_y.cuda()\n",
    "        output = m(content)\n",
    "        loss = loss_func(output, b_y)\n",
    "        if it % 50 == 0:\n",
    "            val_output = m(content_val_tensor)\n",
    "            val_loss = loss_func(val_output, label_val_tensor).cpu().data.numpy().tolist()\n",
    "            print('training loss: ', loss.cpu().data.numpy().tolist())\n",
    "            print('val loss: ', val_loss)\n",
    "            print('training acc: ',accuracy_score(b_y.cpu().data.numpy().tolist(), np.argmax(output.cpu().data.numpy().tolist(), axis=1)))\n",
    "            print('val acc: ', accuracy_score(label_val_tensor.cpu().data.numpy().tolist(), np.argmax(val_output.cpu().data.numpy().tolist(), axis=1)))\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}