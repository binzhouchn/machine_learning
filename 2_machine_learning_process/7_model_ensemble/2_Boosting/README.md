<h1 align = "center">:helicopter: Boosting :running:</h1>

---

## Boosting

 - **基于串行策略**
 
基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成<br>
其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

 - **基本思路**
    - 先从初始训练集训练一个基学习器；初始训练集中各样本的权重是相同的；
    - 根据上一个基学习器的表现，调整样本权重，使分类错误的样本得到更多的关注；
    - 基于调整后的样本分布，训练下一个基学习器；
    - 测试时，对各基学习器加权得到最终结果
 
 - **特点**：每次学习都会使用全部训练样本
 - **代表算法**
    - [AdaBoost算法](https://blog.csdn.net/guyuealian/article/details/70995333)
    - [GBDT算法](https://blog.csdn.net/zpalyq110/article/details/79527653)
    - xgboost
    - lgb

gbdt是利用损失函数的负梯度(一阶导，替代残差)在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树；
，xgb和lgb对损失函数进行了二阶泰勒展开同时用到了一阶导和二阶导；对于最优分割点xgb用贪心算法或近似直方图(主要针对数据太大)，
lgb用的是直方图+双边梯度

[GBDT和xgb的区别](https://www.zhihu.com/question/41354392/answer/98658997)
 - GBDT的基分类器是CART，xgb还支持线性分类器
 - xgb损失函数中加入了正则项
 - 列抽样
 - 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向
 - 支持并行，特征并行
 - 分裂时用近似直方图算法


关于Boosting的两个核心问题：

1. 在每一轮如何改变训练数据的权值或概率分布？

通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

2. 通过什么方式来组合弱分类器？

通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式（使用加权的投票机制代替平均投票机制），即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。


---
## Bagging与Boosting的区别

Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大

3）预测函数：

Bagging：所有预测函数的权重相等

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果；但特征可以并行计算

--- 

## 总结

这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量

下面是将决策树与这些算法框架进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT
