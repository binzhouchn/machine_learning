{"cells":[{"metadata":{"_uuid":"4157d02c56b8135334fe306b846c74cef1044bcd","_cell_guid":"eba83776-8f5f-4f2c-bfab-ed31f15b118f"},"cell_type":"markdown","source":"**Sections**\n\n1. [Introduction](#Introduction)\n\n2. [Data Cleaning](#Data-Cleaning)\n\n3. [Feature Engineering](#Resource-Features)\n\n   3.1. [Resource Features](#Resource-Features)\n   \n   3.2. [Statistical Features](#Statistical-Features)\n   \n   3.3. [Sentimental Analysis](#Sentimental-Analysis)\n   \n   3.4. [Time Features](#Time-Features)\n   \n   3.5. [Polynomial Features](#Polynomial-Features)\n   \n   3.6. [Categorical Features](#Categorical-Features)\n   \n   3.7. [Text Features](#Text-Features)\n   \n4. [Training](#Training-Models)\n\n5. [Output](#Output-for-Test-Set)\n\n6. [Further Possible Improvements](#Further-Possible-Improvements)\n\n\n___\n\n**Update (v14)**\n\n*thanks to [Stranger](https://www.kaggle.com/zyx850525) we fixed a bug in my categorical variables treatment part*\n\n**Update (v11)**\n\n*new treatment for subject categories and sub-categories + new features*\n\n**Update (v8)**\n\n*Included Neural Networks in the learning ensemble*"},{"metadata":{"_uuid":"53f8ff3b8c9db4007d8f099ca48dc289829ee216","_cell_guid":"e4ab4acc-e3c5-4d2b-9a3b-e215a21892fd"},"cell_type":"markdown","source":"# Introduction\n\nHi everybody,\n\nin this notebook, I'm going to present some text and numeric feature extraction techniques. Some of them are already presented in the other kernels and some are new. We will focus mostly on the text and we try to place ourselves in the shoes of grant administrators to see what they might focus on when processing the application, *consciously* or *unconsciously*.\n\nLet's start with importing modules and loading the files:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"import pylab as pl # linear algebra + plots\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport gc\nimport xgboost as xgb\nimport lightgbm as lgb\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import defaultdict, Counter\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom textblob import TextBlob\nfrom scipy.stats import pearsonr\nfrom scipy.sparse import hstack\nfrom multiprocessing import Pool\n\nFolder = \"../input/\"\nTtr = pd.read_csv(Folder + 'train.csv')\nTts = pd.read_csv(Folder + 'test.csv', low_memory=False)\nR = pd.read_csv(Folder + 'resources.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a5e5e5a991f2d00bbeaaf3effafa6157005a59f","_cell_guid":"e3a59fa5-7329-4199-87e6-7839e7091af7"},"cell_type":"markdown","source":"# Data Cleaning\n\nWe know from the data description page that the essay column formats had changed on 2016-05-17, and thereafter, there are only 2 essays; essay 1 matches to the combination of essays 1&2 and new essay 2 is somehow equal to old essays 3&4.\n\nSo, I first move the contents of 'project_essay_2' to 'project_essay_4' when essay 4 is nan, then we simply combine 1&2 and 3&4 to make a uniform dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"# combine the tables into one\ntarget = 'project_is_approved'\nTtr['tr'] = 1; Tts['tr'] = 0\nTtr['ts'] = 0; Tts['ts'] = 1\n\nT = pd.concat((Ttr,Tts))\n\nT.loc[T.project_essay_4.isnull(), ['project_essay_4','project_essay_2']] = \\\n    T.loc[T.project_essay_4.isnull(), ['project_essay_2','project_essay_4']].values\n\nT[['project_essay_2','project_essay_3']] = T[['project_essay_2','project_essay_3']].fillna('')\n\nT['project_essay_1'] = T.apply(lambda row: ' '.join([str(row['project_essay_1']), \n                                                     str(row['project_essay_2'])]), axis=1)\nT['project_essay_2'] = T.apply(lambda row: ' '.join([str(row['project_essay_3']),\n                                                     str(row['project_essay_4'])]), axis=1)\n\nT = T.drop(['project_essay_3', 'project_essay_4'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d80c23f1744e7411d7d222ef02be19358c253a0f","_cell_guid":"87966d65-c8da-4440-a8d5-3124a523667b"},"cell_type":"markdown","source":"# Resource Features\n\nHere we extract some features from the resource file. For each application, there are some resources listed in this file. We can extract how many items and at what prices are requested. minimum, maximum and average price and quantity of each item and for all requested items per application can be important in the decision-making process.\n\nAlso, I combine the resource description columns and make a new text column in table T. Later, we will do text analysis on this column as well."},{"metadata":{"_uuid":"86ffde66ec6502c204ffe1a7fa011c28c9e5f6dd","collapsed":true,"_cell_guid":"3365bf3f-0914-4fae-ace4-bbb07a7d9dbe","trusted":false},"cell_type":"code","source":"R['priceAll'] = R['quantity']*R['price']\nnewR = R.groupby('id').agg({'description':'count',\n                            'quantity':'sum',\n                            'price':'sum',\n                            'priceAll':'sum'}).rename(columns={'description':'items'})\nnewR['avgPrice'] = newR.priceAll / newR.quantity\nnumFeatures = ['items', 'quantity', 'price', 'priceAll', 'avgPrice']\n\nfor func in ['min', 'max', 'mean']:\n    newR = newR.join(R.groupby('id').agg({'quantity':func,\n                                          'price':func,\n                                          'priceAll':func}).rename(\n                                columns={'quantity':func+'Quantity',\n                                         'price':func+'Price',\n                                         'priceAll':func+'PriceAll'}).fillna(0))\n    numFeatures += [func+'Quantity', func+'Price', func+'PriceAll']\n\nnewR = newR.join(R.groupby('id').agg(\n    {'description':lambda x:' '.join(x.values.astype(str))}).rename(\n    columns={'description':'resource_description'}))\n\nT = T.join(newR, on='id')\n\n# if you visit the donors website, it has categorized the price by these bins:\nT['price_category'] = pl.digitize(T.priceAll, [0, 50, 100, 250, 500, 1000, pl.inf])\nnumFeatures.append('price_category')\n# the difference of max and min of price and quantity per item can also be relevant\nfor c in ['Quantity', 'Price', 'PriceAll']:\n    T['max%s_min%s'%(c,c)] = T['max%s'%c] - T['min%s'%c]\n    numFeatures.append('max%s_min%s'%(c,c))\n\ndel Ttr, Tts, R, newR\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81da45ae299a00adce4b3216e28c5445a67b2027","_cell_guid":"b6440e69-e7b6-43d4-ab4b-2be1ce6979ab"},"cell_type":"markdown","source":"# Statistical Features\n\nWe know some teachers have applied many times, and knowing the history of their applications, can be helpful to predict approval. So, I convert the teacher_id to numeric values and include it in my numeric features.\n\nOften times, knowing the statistics of categorical features, i.e. knowing how many times a certain value has repeated in the dataset can help. So let's extract this information:"},{"metadata":{"_uuid":"1399cf655d80034d015f649b70b2b66eb60cf478","collapsed":true,"_cell_guid":"f01b5895-0db0-4a31-b554-52a7f5209a8b","trusted":false},"cell_type":"code","source":"le = LabelEncoder()\nT['teacher_id'] = le.fit_transform(T['teacher_id'])\nT['teacher_gender_unknown'] = T.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))\nnumFeatures += ['teacher_number_of_previously_posted_projects','teacher_id','teacher_gender_unknown']\n\nstatFeatures = []\nfor col in ['school_state', 'teacher_id', 'teacher_prefix', 'teacher_gender_unknown', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'teacher_number_of_previously_posted_projects']:\n    Stat = T[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n    Stat /= Stat.sum()\n    T = T.join(Stat, on=col)\n    statFeatures.append(col+'_stat')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fbce5012aec42cf0380765d5defdf5f4719defb","_cell_guid":"e8827647-f3fa-4346-ab5b-022d5fbbe8e0"},"cell_type":"markdown","source":"# Sentimental Analysis\n\nWith the help of textblob module, we can find polarity and subjectivity of texts to some extent. It is, unfortunately, a little time-consuming. There might be other modules that work faster like [VADER-Sentiment](https://github.com/cjhutto/vaderSentiment). Though, I haven't checked other modules. Their quality of analysis can also be different. Have you ever tried other modules? Do you know any better one?\n\nAnother way of doing (sort of) sentimental analysis is to check for certain words and characters in the texts. I, personally, for example, feel uncomfortable if a text has so many exclamation marks :D. But, seriously, some of these may have an unconscious effect on the examiner. For example, if any words are bolded by \", or the number of sentences (number of \".\"), number of paragraphs (\\r), talking about money ($) or percentages (%), having a URL (http), etc. can influence the decision. What other words or characters do you think can be important?\n\nTalking about **I** or **WE** and having positive or negative words and phrases like that can also be influential. In one of the following sections (**Text Features**), by extracting n-grams, I hope to catch such phrases if they appear as repeated patterns.\n\nThe number of words or the length of the texts can be another factor that can influence the decision unconsciously (or even consciously!). Number of transitional words, verbs, adjectives, adverbs, etc. in an essay can also indicate some aspects of the quality of the text.\n\nBut, certainly, the quality of the essays is the most effective factor in my opinion. Things like the grammar errors, spelling errors, quality of the texts, word choices etc. are very important. Another important factor, if I was a grant examiner, would have been to check if the application writer could relate their needs to the resources they want through essays and project title. One primitive way to do this is to check for common words in different texts. Let me know if you know any better way to do these type of analysis."},{"metadata":{"_uuid":"108fd127c916a493748c25d83d92ce4d1b00a5a8","collapsed":true,"_cell_guid":"3743dc74-cd95-4b20-bf3c-6b2e3caef34f","trusted":false},"cell_type":"code","source":"%%time\ntextColumns = ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'resource_description', 'project_title']\n\ndef getSentFeat(s):\n    sent = TextBlob(s).sentiment\n    return (sent.polarity, sent.subjectivity)\n\nprint('sentimental analysis')\nwith Pool(4) as p:\n    for col in textColumns:\n        temp = pl.array(list(p.map(getSentFeat, T[col])))\n        T[col+'_pol'] = temp[:,0]\n        T[col+'_sub'] = temp[:,1]\n        numFeatures += [col+'_pol', col+'_sub']\n\nprint('key words')\nKeyChars = ['!', '\\?', '@', '#', '\\$', '%', '&', '\\*', '\\(', '\\[', '\\{', '\\|', '-', '_', '=', '\\+',\n            '\\.', ':', ';', ',', '/', '\\\\\\\\r', '\\\\\\\\t', '\\\\\"', '\\.\\.\\.', 'etc', 'http', 'poor',\n            'military', 'traditional', 'charter', 'head start', 'magnet', 'year-round', 'alternative',\n            'art', 'book', 'basics', 'computer', 'laptop', 'tablet', 'kit', 'game', 'seat',\n            'food', 'cloth', 'hygiene', 'instraction', 'technolog', 'lab', 'equipment',\n            'music', 'instrument', 'nook', 'desk', 'storage', 'sport', 'exercise', 'trip', 'visitor',\n            'my students', 'our students', 'my class', 'our class']\nfor col in textColumns:\n    for c in KeyChars:\n        T[col+'_'+c] = T[col].apply(lambda x: len(re.findall(c, x.lower())))\n        numFeatures.append(col+'_'+c)\n\n#####\nprint('num words')\nfor col in textColumns:\n    T['n_'+col] = T[col].apply(lambda x: len(x.split()))\n    numFeatures.append('n_'+col)\n    T['nUpper_'+col] = T[col].apply(lambda x: sum([s.isupper() for s in list(x)]))\n    numFeatures.append('nUpper_'+col)\n\n#####\nprint('word tags')\nTags = ['CC', 'CD', 'DT', 'IN', 'JJ', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', \n        'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', \n        'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\ndef getTagFeat(s):\n    d = Counter([t[1] for t in pos_tag(s.split())])\n    return [d[t] for t in Tags]\n\nwith Pool(4) as p:\n    for col in textColumns:\n        temp = pl.array(list(p.map(getTagFeat, T[col])))\n        for i, t in enumerate(Tags):\n            if temp[:,i].sum() == 0:\n                continue\n            T[col+'_'+t] = temp[:, i]\n            numFeatures += [col+'_'+t]\n\n#####\nprint('common words')\nfor i, col1 in enumerate(textColumns[:-1]):\n    for col2 in textColumns[i+1:]:\n        T['%s_%s_common' % (col1, col2)] = T.apply(lambda row:len(set(re.split('\\W', row[col1].lower())).intersection(re.split('\\W', row[col2].lower()))), axis=1)\n        numFeatures.append('%s_%s_common' % (col1, col2))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"671c946dea8a70e0146abc3888e535196eae590b","_cell_guid":"3aac0f1e-dfb7-4fec-b4dd-27cb7e668d3b"},"cell_type":"markdown","source":"Guess what! someone didn't like **!**s in essays. "},{"metadata":{"_uuid":"4d1c1b51ce99b7199b7eb450900a0c53a8c0cdd5","collapsed":true,"_cell_guid":"c86749c3-0bf9-4675-a452-64ecc53128d8","trusted":false},"cell_type":"code","source":"pl.figure(figsize=(15,5))\nsns.violinplot(data=T,x=target,y='project_essay_2_!');\npl.figure(figsize=(15,5))\nsns.violinplot(data=T,x=target,y='project_essay_1_!');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dbd227646a55ef49ca671ef3f3d4ae0ae8f9c5b","_cell_guid":"21ccce00-851a-4327-a299-1008ecfb6451"},"cell_type":"markdown","source":"# Time Features\n\nThe time at which the proposal was submitted can be important. Most importantly, we know thanks to [Heads or Tails](https://www.kaggle.com/headsortails/an-educated-guess-update-feature-engineering) that there is a slight approval rate modulation over time. So we need to extract date info. Day of the week it has been posted can also play a role. I doubt if the hour it was submitted has any significance, but let's let the decision trees take care of that. Next, let's extract some statistics from time features as well."},{"metadata":{"_uuid":"c97734d54aaf7c667f23c39fd80588f4e8860521","collapsed":true,"_cell_guid":"17956b51-6093-4d45-9155-9def94f9f8ae","trusted":false},"cell_type":"code","source":"dateCol = 'project_submitted_datetime'\ndef getTimeFeatures(T):\n    T['year'] = T[dateCol].apply(lambda x: x.year)\n    T['month'] = T[dateCol].apply(lambda x: x.month)\n    T['day'] = T[dateCol].apply(lambda x: x.day)\n    T['dow'] = T[dateCol].apply(lambda x: x.dayofweek)\n    T['hour'] = T[dateCol].apply(lambda x: x.hour)\n    T['days'] = (T[dateCol]-T[dateCol].min()).apply(lambda x: x.days)\n    return T\n\nT[dateCol] = pd.to_datetime(T[dateCol])\nT = getTimeFeatures(T)\n\nP_tar = T[T.tr==1][target].mean()\ntimeFeatures = ['year', 'month', 'day', 'dow', 'hour', 'days']\nfor col in timeFeatures:\n    Stat = T[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_stat'})\n    Stat /= Stat.sum()\n    T = T.join(Stat, on=col)\n    statFeatures.append(col+'_stat')\n\nnumFeatures += timeFeatures\nnumFeatures += statFeatures","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3c1abe31781a1764c269c1b5c3455ae70b87ee2","_cell_guid":"ff8aed0a-93c2-4708-a52b-1568c7ed629a"},"cell_type":"markdown","source":"# Polynomial Features\n\nSo far, I have extracted some numerical features. Often it helps the decision trees to provide some polynomial features to them. Here I include first-order interaction polynomials, and I check for the significance of the new variable before adding it to the columns. I add it only if it really helps to predict the approval better. A trick that I'm using here is that, maybe, the division of two variables is more significantly predicting the target! That would be the case if 1/V is a more significant predictor than V. So, I check for the significance of 1/(V+1) and V+1 (+1 is to avoid production or division by 0), and replace the most significant one to the original variable V. What do you think about this? It certainly helped though!\n\nBy checking the significance and correlation in training set, there will be an over-training chance, which I'm trying to decrease by computing the average of correlations and p-values over randomly selected subsets."},{"metadata":{"_kg_hide-output":true,"collapsed":true,"_uuid":"35e5318c809a7ecb64165d9e1a20187cfe55cf1a","_cell_guid":"1b5641c3-dc41-47e5-bd87-b69e99593fa3","trusted":false},"cell_type":"code","source":"%%time\nT2 = T[numFeatures+['id','tr','ts',target]].copy()\nTtr = T2[T.tr==1]\nTar_tr = Ttr[target].values\nn = 10\ninx = [pl.randint(0, Ttr.shape[0], int(Ttr.shape[0]/n)) for k in range(n)]\n# inx is used for crossvalidation of calculating the correlation and p-value\nCorr = {}\nfor c in numFeatures:\n    # since some values might be 0s, I use x+1 to avoid missing some important relations\n    C1,P1=pl.nanmean([pearsonr(Tar_tr[inx[k]],   (1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n    C2,P2=pl.nanmean([pearsonr(Tar_tr[inx[k]], 1/(1+Ttr[c].iloc[inx[k]])) for k in range(n)], 0)\n    if P2<P1:\n        T2[c] = 1/(1+T2[c])\n        Corr[c] = [C2,P2]\n    else:\n        T2[c] = 1+T2[c]\n        Corr[c] = [C1,P1]\n\npolyCol = []\nthrP = 0.01\nthrC = 0.02\nprint('columns \\t\\t\\t Corr1 \\t\\t Corr2 \\t\\t Corr Combined')\nfor i, c1 in enumerate(numFeatures[:-1]):\n    C1, P1 = Corr[c1]\n    for c2 in numFeatures[i+1:]:\n        C2, P2 = Corr[c2]\n        V = T2[c1] * T2[c2]\n        Vtr = V[T2.tr==1].values\n        C, P = pl.nanmean([pearsonr(Tar_tr[inx[k]], Vtr[inx[k]]) for k in range(n)], 0)\n        if P<thrP and abs(C) - max(abs(C1),abs(C2)) > thrC:\n            T[c1+'_'+c2+'_poly'] = V\n            polyCol.append(c1+'_'+c2+'_poly')\n            print(c1+'_'+c2, '\\t\\t(%g, %g)\\t(%g, %g)\\t(%g, %g)'%(C1,P1, C2,P2, C,P))\n\nnumFeatures += polyCol\nprint(len(numFeatures))\ndel T2, Ttr\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78382c455955fb9baf63176dd5daeeefd7774409","_cell_guid":"b54d5fc2-e37a-48b6-bae2-d8dbeaa6ff05"},"cell_type":"markdown","source":"For example, the variable created out of *maxPrice* and *meanPrice* is much more informative:"},{"metadata":{"_uuid":"a1b0416d872c3af089241a5e669b360d98b34e71","collapsed":true,"_cell_guid":"aa247108-204c-4f3a-b904-cad7df7cd6cd","trusted":false},"cell_type":"code","source":"pl.figure(figsize=(15,5));sns.violinplot(data=T,x=target,y='maxPrice')\npl.figure(figsize=(15,5));sns.violinplot(data=T,x=target,y='meanPrice')\npl.figure(figsize=(15,5));sns.violinplot(data=T,x=target,y='maxPrice_meanPrice_poly');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f03893f165cf128033bbcaa365bbdbe9513fc79b","_cell_guid":"0247184d-dd28-49c7-9104-f15326f3efc9"},"cell_type":"markdown","source":"# Categorical Features\n\nNext, we include categorical features. Categorical features are teacher prefix, state, grade, and subject categories. We previously were doing it wrong by taking onehot or label encoder, in particular for project subject categorirs and sub-categories. For example, \"Performing Arts, Team Sports\" is in fact a combination of two categories: \"Performing Arts\" and \"Team Sports\" which previously we were taking this combination as a sole category. I've fixed it here by using count vectorizers. Now we have 9 categories and 30 sub-categories. It should be actually 8 categories since \"Warmth, Care & Hunger\" is one category which our algorithm takes it as two categories when it splits by comma, but it doesn't affect our results dramatically."},{"metadata":{"_uuid":"a57a9b4ea3cce08cd1b69134e7cc8bbd5f1b9f51","collapsed":true,"_cell_guid":"45e61bea-990c-4ca8-ae0e-7efe611d6971","trusted":false},"cell_type":"code","source":"def getCatFeatures(T, Col):\n    vectorizer = CountVectorizer(binary=True,\n                                 ngram_range=(1,1),\n                                 tokenizer=lambda x:[a.strip() for a in x.split(',')])\n    return vectorizer.fit_transform(T[Col].fillna(''))\n\nX_tp = getCatFeatures(T, 'teacher_prefix')\nX_ss = getCatFeatures(T, 'school_state')\nX_pgc = getCatFeatures(T, 'project_grade_category')\nX_psc = getCatFeatures(T, 'project_subject_categories')\nX_pssc = getCatFeatures(T, 'project_subject_subcategories')\n\nX_cat = hstack((X_tp, X_ss, X_pgc, X_psc, X_pssc))\n\ndel X_tp, X_ss, X_pgc, X_psc, X_pssc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44c22f1eadfcaad5c00879ddf733cd9848257b81","_cell_guid":"e4f63065-120c-428b-af6a-38e2a82b34e9"},"cell_type":"markdown","source":"# Text Features\n\nFinally, we do text analysis. For this section, I used both Tf-IDF and count vectorizer and interestingly, count vectorizer with binary features, showing only if a word is in the text, has the best performance in my experience. Other than that, since there are mis-spellings in the texts, it would have helped to check for spelling errors first. I found \"TextBlob\" and \"autocorrect\" modules for this purpose but, unfortunately, it was so slow and I didn't use it at last. Do you know any better way to do that?Also, I decided not using any stop words because some of them can actually be useful in this case and after all they are only a few words.\n\nI tried using dimensionality reduction techniques to reduce the dimensions following the idea of Latent Semantic Analysis, but it didn't help the prediction as well."},{"metadata":{"_uuid":"f1e87bc07dc73a9a2e78ba22c408f3848dbb04c4","collapsed":true,"_cell_guid":"ce511c83-9b16-4bea-875d-18f5690bdfe0","trusted":false},"cell_type":"code","source":"%%time\n# from nltk.stem.wordnet import WordNetLemmatizer\n# from autocorrect import spell  # as spell checker and corrector\n# L = WordNetLemmatizer()\np = PorterStemmer()\ndef wordPreProcess(sentence):\n    return ' '.join([p.stem(x.lower()) for x in re.split('\\W', sentence) if len(x) >= 1])\n# return ' '.join([p.stem(L.lemmatize(spell(x.lower()))) for x in re.split('\\W', sentence) if len(x) > 1])\n\n\ndef getTextFeatures(T, Col, max_features=10000, ngrams=(1,2), verbose=True):\n    if verbose:\n        print('processing: ', Col)\n    vectorizer = CountVectorizer(stop_words=None,\n                                 preprocessor=wordPreProcess,\n                                 max_features=max_features,\n                                 binary=True,\n                                 ngram_range=ngrams)\n#     vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'),\n#                                  preprocessor=wordPreProcess,\n#                                  max_features=max_features)\n    X = vectorizer.fit_transform(T[Col])\n    return X, vectorizer.get_feature_names()\n\nn_es1, n_es2, n_prs, n_rd, n_pt = 3000, 8000, 2000, 3000, 1000\nX_es1, feat_es1 = getTextFeatures(T, 'project_essay_1', max_features=n_es1)\nX_es2, feat_es2 = getTextFeatures(T, 'project_essay_2', max_features=n_es2)\nX_prs, feat_prs = getTextFeatures(T, 'project_resource_summary', max_features=n_prs)\nX_rd, feat_rd = getTextFeatures(T, 'resource_description', max_features=n_rd, ngrams=(1,3))\nX_pt, feat_pt = getTextFeatures(T, 'project_title', max_features=n_pt)\n\nX_txt = hstack((X_es1, X_es2, X_prs, X_rd, X_pt))\ndel X_es1, X_es2, X_prs, X_rd, X_pt\n\n# \n# from sklearn.decomposition import TruncatedSVD\n# svd = TruncatedSVD(1000)\n# X_txt = svd.fit_transform(X_txt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc68c82887f94acf062bac04a44c35e85ce7e4a8","_cell_guid":"fbd7c462-10ad-483d-afb6-27211b30005e"},"cell_type":"markdown","source":"Finally, let's make up the train and test matrices:\n\nwe should normalize the values if we want to use neural networks. Since my sparse features are 0s and 1s, I only apply it to numerical values."},{"metadata":{"_uuid":"baf065d495812fb174a157f9257dfef0f9880fd1","collapsed":true,"_cell_guid":"62487f21-f3c3-4eaa-9a69-33e7c394b663","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = hstack((X_txt, X_cat, StandardScaler().fit_transform(T[numFeatures].fillna(0)))).tocsr()\n\nXtr = X[pl.find(T.tr==1), :]\nXts = X[pl.find(T.ts==1), :]\nTtr_tar = T[T.tr==1][target].values\nTts = T[T.ts==1][['id',target]]\n\nYts = []\ndel T\ndel X\ngc.collect();\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"822836adc80aaa3142743a2e0f99fb54bc2bb0ac","_cell_guid":"f08180b8-87c9-4f7e-890b-96b8c8cbb60f"},"cell_type":"markdown","source":"# Training Models\n\nHere I train XGB, LGB and NN models and stack them. I have two levels of stacking. First level on the results of learners by LinearRegression. The second level is by simple average over the results taken by different validation sets. In this version of the kernel I do it only for one validation set because of the time limit, but the result will be of course better if we stack more models. My NN model is inspired by [huiqin's kernel](https://www.kaggle.com/qinhui1999/deep-learning-is-all-you-need-lb-0-80x). Concisely, the structure of my network is like this:\n\n![NN structure](https://i.imgur.com/bkvRySn.jpg)"},{"metadata":{"_uuid":"7dca28fb44545b8e9d7d31e8166a8c55c8f20bd2","collapsed":true,"_cell_guid":"8e1cecfa-0de5-4055-8234-a65432f1cc78","trusted":false},"cell_type":"code","source":"from keras.layers import Input, Dense, Flatten, concatenate, Dropout, Embedding, SpatialDropout1D\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.models import Model\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef breakInput(X1):\n    X2 = []\n    i = 0\n    for n in [n_es1, n_es2, n_prs, n_rd, n_pt, X_cat.shape[1], len(numFeatures)]:\n        X2.append(X1[:,i:i+n])\n        i += n\n    return X2\n\ndef getModel(HLs, Drop=0.25, OP=optimizers.Adam()):\n    temp = []\n    inputs_txt = []\n    for n in [n_es1, n_es2, n_prs, n_rd, n_pt]:\n        input_txt = Input((n, ))\n        X_feat = Dropout(Drop)(input_txt)\n        X_feat = Dense(int(n/100), activation=\"linear\")(X_feat)\n        X_feat = Dropout(Drop)(X_feat)\n        temp.append(X_feat)\n        inputs_txt.append(input_txt)\n\n    x_1 = concatenate(temp)\n#     x_1 = Dense(20, activation=\"relu\")(x_1)\n    x_1 = Dense(50, activation=\"relu\")(x_1)\n    x_1 = Dropout(Drop)(x_1)\n\n    input_cat = Input((X_cat.shape[1], ))\n    x_2 = Embedding(2, 10, input_length=X_cat.shape[1])(input_cat)\n    x_2 = SpatialDropout1D(Drop)(x_2)\n    x_2 = Flatten()(x_2)\n\n    input_num = Input((len(numFeatures), ))\n    x_3 = Dropout(Drop)(input_num)\n    \n    x = concatenate([x_1, x_2, x_3])\n\n    for HL in HLs:\n        x = Dense(HL, activation=\"relu\")(x)\n        x = Dropout(Drop)(x)\n\n    output = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inputs_txt+[input_cat, input_num], outputs=output)\n    model.compile(\n            optimizer=OP,\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy'])\n    return model\n\ndef trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam()):\n    file_path='NN.h5'\n    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)\n    lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n                                   factor=0.5,\n                                   patience=2,\n                                   verbose=1,\n                                   epsilon=3e-4,\n                                   mode='min')\n\n    model = getModel(HL, Drop, OP)\n    model.fit(breakInput(X_train), Tar_train, validation_data=(breakInput(X_val), Tar_val),\n                        verbose=2, epochs=50, batch_size=1000, callbacks=[early, lr_reduced, checkpoint])\n    model.load_weights(file_path)\n    return model\n\nparams_xgb = {\n        'eta': 0.05,\n        'max_depth': 4,\n        'subsample': 0.85,\n        'colsample_bytree': 0.25,\n        'min_child_weight': 3,\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'seed': 0,\n        'silent': 1,\n    }\nparams_lgb = {\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 10,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.25,\n        'bagging_fraction': 0.85,\n        'seed': 0,\n        'verbose': 0,\n    }\nnCV = 1 # should be ideally larger\nfor i in range(21, 22):\n    gc.collect()\n    X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n    # XGB\n    dtrain = xgb.DMatrix(X_train, label=Tar_train)\n    dval   = xgb.DMatrix(X_val, label=Tar_val)\n    watchlist = [(dtrain, 'train'), (dval, 'valid')]\n    model = xgb.train(params_xgb, dtrain, 5000,  watchlist, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n    Yvl1 = model.predict(dval)\n    Yts1 = model.predict(xgb.DMatrix(Xts))\n    # LGB\n    dtrain = lgb.Dataset(X_train, Tar_train)\n    dval   = lgb.Dataset(X_val, Tar_val)\n    model = lgb.train(params_lgb, dtrain, num_boost_round=10000, valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=200)\n    Yvl2 = model.predict(X_val)\n    Yts2 = model.predict(Xts)\n    # NN\n    model = trainNN(X_train, X_val, Tar_train, Tar_val, HL=[50], Drop=0.5, OP=optimizers.Adam())\n    Yvl3 = model.predict(breakInput(X_val)).squeeze()\n    Yts3 = model.predict(breakInput(Xts)).squeeze()\n    # stack\n    M = LinearRegression()\n    M.fit(pl.array([Yvl1, Yvl2, Yvl3]).T, Tar_val)\n    Yts.append(M.predict(pl.array([Yts1, Yts2, Yts3]).T))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57dd1713e47073615b6af9b8c8e5e3f5ccc882b2","_cell_guid":"373c429f-3e09-44ce-80c2-3fcd1802a0ab"},"cell_type":"markdown","source":"# Output for Test Set\n\nAt last, we make the stack of test set outputs by simple averaging, maybe rank average or median work better, I didn't try."},{"metadata":{"_uuid":"945b110c20cf8db3f53b61e987b65c3e53ac197a","collapsed":true,"_cell_guid":"7b149c3f-3c06-432c-a2de-49c577cf8f93","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nTts[target] = MinMaxScaler().fit_transform(pl.array(Yts).mean(0).reshape(-1,1))\nTts[['id', target]].to_csv('text_cat_num_xgb_lgb_NN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a9ee175fd3bc8d9f3149953e8e713448b77a8f7","_cell_guid":"044bd3dc-9435-4be1-ad8d-2d4753b0541b"},"cell_type":"markdown","source":"# Further Possible Improvements\n\n* One obvious way to improve it is to play with the decision tree parameters, stacking more models, and perhaps stacking different kinds of learners\n* Fluency and articulation of the texts can be an important factor if we could somehow measure it\n* Checking for existence of special keywords that might attract or repel the reader -- if they are not already captured by the extracted n-grams\n* Checking and correcting for the spell of the words, before stemming them can help. Also, maybe the existence of spell or grammatical errors influences the decision\n* Checking for concurrency of the texts or required resources to special events that have occurred at that time might be useful. Maybe because of some events, some proposals are being accepted more easily, due to public awareness or hotness of some topics\n\nThanks for staying so far! Hope it helps.\n\nLet me know if you have any comments, or suggestions for improvement, or if you think I can do some parts more efficiently."}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}