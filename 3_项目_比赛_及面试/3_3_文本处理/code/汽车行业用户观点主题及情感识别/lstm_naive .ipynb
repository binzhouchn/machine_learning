{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "from m import f1_for_car, BOW, BasicModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:39.131689Z",
     "start_time": "2018-10-15T12:14:39.075309Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题和情感合起来变成30类\n",
    "data['subject_1'] = data['subject'] + data['sentiment_value'].astype(str)\n",
    "subj_lst = list(filter(lambda x : x is not np.nan, list(set(data.subject_1))))\n",
    "subj_lst_dic = {value:key for key, value in enumerate(subj_lst)}\n",
    "data['label'] = data['subject_1'].apply(lambda x : subj_lst_dic.get(x))\n",
    "\n",
    "data = data[['content', 'label']].copy(deep=True)\n",
    "data_tmp = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:45.837140Z",
     "start_time": "2018-10-15T12:14:45.826930Z"
    }
   },
   "outputs": [],
   "source": [
    "# subj_dict = {'价格':['价格','性价比','低价','降价','优惠','便宜','划算','不菲','实惠','贵','价差','单价','合算','合理','高昂','有钱任性','保值率','费用','同价位','评估价','最高配','最低配','前（钱）紧','8万'],\\\n",
    "#             '油耗':['油耗','高速','市区','公里','废油','不见得省','省油','个油','节油','机油','油号','费油','不省什么油'],\\\n",
    "#             '配置':['配置','导航','视野','倒车雷达','倒车影像','中控','后视镜','自动泊车','摄像头','前雷达','车载','音质','背光','简配','落锁','出风口'],\\\n",
    "#             '内饰':['内饰','氛围','单调','寒酸','用料','细致','设计感','异味','做工','简陋','粗糙','档次','不够整','劣质材料','防火材料'],\\\n",
    "#             '操控':['操控','控制','偏硬','不费劲','迟钝','底盘','操纵','减震','方向盘','尾排','加强件','刹车','灵活','韧性','漂移','手感差','变速箱','平衡性'],\\\n",
    "#             '空间':['空间','视野','舒服','容量','显小','钻进去','宽敞','宽大','轴距','车体'],\\\n",
    "#             '外观':['外观','杀马特','大气','前脸','外形','变色','漆面','油漆','车漆','眼缘','尾灯','帅气','镀铬','镀络','颜值','挺炫','屁股','新潮','里外不一','好看','颜色','寒冰银','蓝色','黑色','不耐脏','银色','红色','蓝棕','黄贴'],\\\n",
    "#             '动力':['动力','驱动','发动机','机油','散热','四驱','强劲','变速箱','飙车','爆缸','排量','尾排','爬坡','油门踩到底','怕烧机油'],\\\n",
    "#             '安全性':['安全','刹车','手刹','追尾','气囊','加速','扎实','防爆胎','被盗','防盗','失去抓地力'],\\\n",
    "#             '舒适性':['舒适','隔音','舒服','噪音','异响','吵','静音','风噪','都会响','出风口','安静','空调','气门','颈椎','累','制冷','恒温','声音','抖','座椅','视野','宽大','晕车','减震','腰疼','卡顿','坐姿','颠簸','气味','滴水','后备箱响']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试一下用LSTM进行主题分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "USE_CUDA=True\n",
    "EPOCH = 30           # 训练整批数据多少次\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.002         # 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:10.663832Z",
     "start_time": "2018-10-15T12:15:03.802679Z"
    }
   },
   "outputs": [],
   "source": [
    "d_ = {}\n",
    "for key, value in enumerate(set(data_tmp.label)):\n",
    "    d_[value] = key\n",
    "data_tmp['label'] = data_tmp['label'].apply(lambda x : d_.get(x))\n",
    "\n",
    "y_train = np.array(data_tmp.label.tolist())\n",
    "# 构造embedding字典\n",
    "bow = BOW(data_tmp.content.apply(jieba.lcut).tolist(), min_count=1, maxlen=30) # 长度补齐或截断固定长度30\n",
    "\n",
    "vocab_size = len(bow.word2idx)\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('data/ft_wv.txt')\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size+1, 300))\n",
    "for key, value in bow.word2idx.items():\n",
    "    if key in word2vec.vocab: # Word2Vec训练得到的的实例需要word2vec.wv.vocab\n",
    "        embedding_matrix[value] = word2vec.get_vector(key)\n",
    "    else:\n",
    "        embedding_matrix[value] = [0] * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:56:35.081377Z",
     "start_time": "2018-10-15T12:56:35.070324Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('save/embedding_matrix', arr=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:34.585011Z",
     "start_time": "2018-10-15T12:15:34.577761Z"
    }
   },
   "outputs": [],
   "source": [
    "# word对应的index\n",
    "X_train = copy.deepcopy(bow.doc2num)\n",
    "y_train = copy.deepcopy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:18.996896Z",
     "start_time": "2018-10-16T02:35:18.983656Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "\n",
    "    loss = 'multilabelloss'\n",
    "    model='LSTMText' \n",
    "    num_classes = 30 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 1000 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2 # k\n",
    "    hidden_size = 128 #LSTM hidden size\n",
    "    num_layers=2 #LSTM layers\n",
    "    inception_dim = 256 #inception的卷积核数\n",
    "    \n",
    "    # vocab_size = 11973 # num of chars\n",
    "    vocab_size = vocab_size # num of words \n",
    "    content_seq_len = 100 #描述长度 word为100 char为200\n",
    "    static = False\n",
    "    embedding_path = 'save/embedding_matrix.npy'\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:19.750470Z",
     "start_time": "2018-10-16T02:35:19.700539Z"
    }
   },
   "outputs": [],
   "source": [
    "# 相当于把seq_len压缩成k个'词'\n",
    "# dim共三个维度，这里取2即seq_len那个维度，100->k\n",
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class LSTMText(BasicModule): \n",
    "    def __init__(self, opt):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.model_name = 'LSTMText'\n",
    "        self.opt=opt\n",
    "\n",
    "        kernel_size = self.opt.kernel_size\n",
    "        self.encoder = torch.nn.Embedding(self.opt.vocab_size+1, self.opt.embedding_dim)\n",
    "\n",
    "        self.content_lstm =torch.nn.LSTM(input_size = self.opt.embedding_dim,\\\n",
    "                            hidden_size = self.opt.hidden_size,\n",
    "                            num_layers = self.opt.num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            dropout = 0.5, # dropout\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.opt.kmax_pooling*(self.opt.hidden_size*2),self.opt.linear_hidden_size),\n",
    "            torch.nn.Dropout(0.2), # dropout\n",
    "            torch.nn.BatchNorm1d(self.opt.linear_hidden_size),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(self.opt.linear_hidden_size, self.opt.num_classes),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "        if self.opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(self.opt.embedding_path)))\n",
    " \n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "        # torch.Size([64, 100, 150])\n",
    "        if self.opt.static:\n",
    "            title=title.detach()\n",
    "            content=content.detach()\n",
    "        \n",
    "        '''\n",
    "        lstm输入的时候需要转成(seq_len, batch, embedding_dim）这种维度（用permute转）<br>\n",
    "        output，每个时刻的LSTM网络的最后一层的输出，维度（seq_len, batch, hidden_size * num_directions）|双向lstm所以输出的hidden_size维度要乘以2<br>\n",
    "        lstm的输出为output, (hn, cn) 的元组<br>\n",
    "        这里取第一个就是output(100,64,256)，第二个是元组其中的第一个hn就是最后时刻的隐层状态hn(4,64,128)\n",
    "        这里的4就是(2层num_layers*双向)lstm得到\n",
    "        '''\n",
    "        content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n",
    "        #torch.Size([64, 256, 100])\n",
    "        content_conv_out = kmax_pooling((content_out),2,self.opt.kmax_pooling)\n",
    "        conv_out = content_conv_out\n",
    "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
    "        softmax = self.fc((reshaped))\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:25.730132Z",
     "start_time": "2018-10-16T02:35:25.719742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理成tensor\n",
    "label_tensor = torch.from_numpy(np.array(y_train)).long()\n",
    "content_tensor = torch.from_numpy(np.array(X_train)).long()\n",
    "\n",
    "torch_dataset = Data.TensorDataset(content_tensor, label_tensor)\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # random shuffle for training\n",
    "        num_workers=8,              # subprocesses for loading data\n",
    "    )\n",
    "\n",
    "# 如果需要验证集则可以将X_train进行拆分\n",
    "\n",
    "# model, optimizer, loss_func\n",
    "m = LSTMText(opt)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=LR)   # optimize all lstm parameters;Adam比较好用\n",
    "loss_func = torch.nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "if USE_CUDA:\n",
    "    m.cuda()\n",
    "    loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # val\n",
    "# if USE_CUDA:\n",
    "#     content_val_tensor = content_val_tensor.cuda()\n",
    "#     label_val_tensor = label_val_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-16T02:35:27.518Z"
    }
   },
   "outputs": [],
   "source": [
    "it = 1\n",
    "for epoch in tqdm_notebook(range(EPOCH)):\n",
    "    for step, (content, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "        content, b_y = content.cuda(), b_y.cuda()\n",
    "        output = m(content)\n",
    "        loss = loss_func(output, b_y)\n",
    "        if it % 50 == 0:\n",
    "            val_output = m(content_val_tensor)\n",
    "            val_loss = loss_func(val_output, label_val_tensor).cpu().data.numpy().tolist()\n",
    "            print('training loss: ', loss.cpu().data.numpy().tolist())\n",
    "            print('val loss: ', val_loss)\n",
    "            print('training acc: ',accuracy_score(b_y.cpu().data.numpy().tolist(), np.argmax(output.cpu().data.numpy().tolist(), axis=1)))\n",
    "            print('val acc: ', accuracy_score(label_val_tensor.cpu().data.numpy().tolist(), np.argmax(val_output.cpu().data.numpy().tolist(), axis=1)))\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        it += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}